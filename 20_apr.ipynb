{
 "cells": [
  {
   "cell_type": "raw",
   "id": "4e1b58d7",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?\n",
    "Q2. How do you choose the value of K in KNN?\n",
    "Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "Q4. How do you measure the performance of KNN?\n",
    "Q5. What is the curse of dimensionality in KNN?\n",
    "Q6. How do you handle missing values in KNN?\n",
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?\n",
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?\n",
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dafb7b",
   "metadata": {},
   "source": [
    "1."
   ]
  },
  {
   "cell_type": "raw",
   "id": "df4a7011",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?\n",
    "\n",
    "The K-Nearest Neighbors (KNN) algorithm is a simple and versatile supervised machine learning algorithm used for both classification and regression tasks. In KNN, an object is classified or predicted by the majority class or average of its k-nearest neighbors in the feature space. The distance metric (usually Euclidean distance) is used to measure the proximity between data points."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ab2c3b7b",
   "metadata": {},
   "source": [
    "\n",
    "Q2. How do you choose the value of K in KNN?\n",
    "\n",
    "\n",
    "First of all K is hyperparameter , its value is chosen by cross validation .\n",
    "The choice of the value of K is crucial in KNN. A small value of K may lead to noise sensitivity, while a large value may result in over-smoothing. Common methods for choosing K include:\n",
    "\n",
    "Trying various values and using cross-validation to select the one that minimizes error.\n",
    "Using the square root of the number of data points as a rule of thumb.\n",
    "Experimenting with odd values of K to avoid ties in binary classification problems."
   ]
  },
  {
   "cell_type": "raw",
   "id": "649325ad",
   "metadata": {},
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "KNN Classifier: It predicts the class label by a majority vote of its k-nearest neighbors. The output is a class membership.\n",
    "\n",
    "KNN Regressor: It predicts the output variable by averaging or taking the majority vote of the target values of its k-nearest neighbors. The output is a continuous value."
   ]
  },
  {
   "cell_type": "raw",
   "id": "dca53259",
   "metadata": {},
   "source": [
    "Q4. How do you measure the performance of KNN?\n",
    "\n",
    "Common performance metrics for KNN include accuracy, precision, recall, F1-score (for classification), and Mean Squared Error (MSE) or R-squared (for regression)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e72311f0",
   "metadata": {},
   "source": [
    "Q5. What is the curse of dimensionality in KNN?\n",
    "\n",
    "The curse of dimensionality refers to the challenges that arise when working with high-dimensional data. In KNN, as the number of dimensions increases, the distance between points becomes more uniform, making it harder to identify close neighbors. This can lead to increased computational complexity and reduced effectiveness of KNN in high-dimensional spaces."
   ]
  },
  {
   "cell_type": "raw",
   "id": "cb4b20a7",
   "metadata": {},
   "source": [
    "Q6. How do you handle missing values in KNN?\n",
    "\n",
    "Imputation techniques such as replacing missing values with the mean, median, or mode of the available values in the feature can be applied. Another approach is to use KNN imputation, where the missing value is estimated based on the values of its k-nearest neighbors"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aea995a3",
   "metadata": {},
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
    "\n",
    "Classifier: Suitable for problems where the output is a discrete class label, e.g., image classification, spam detection.\n",
    "\n",
    "Regressor: Suitable for problems where the output is a continuous value, e.g., predicting house prices, stock prices"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d30d9ca9",
   "metadata": {},
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
    "\n",
    "Strengths: Simple, non-parametric, effective for small datasets.\n",
    "Weaknesses: Sensitive to irrelevant features, computationally expensive for large datasets.\n",
    "Addressing Weaknesses: Feature selection, dimensionality reduction, using efficient data structures (e.g., KD-trees) for faster search."
   ]
  },
  {
   "cell_type": "raw",
   "id": "61a47828",
   "metadata": {},
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\n",
    "Euclidean Distance: Represents the straight-line distance between two points in the feature space.\n",
    "\n",
    "Manhattan Distance: Represents the sum of the absolute differences between corresponding coordinates of two points."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4c6c3cbe",
   "metadata": {},
   "source": [
    "Q10. What is the role of feature scaling in KNN?\n",
    "\n",
    "Feature scaling ensures that all features contribute equally to the distance computation. Since KNN relies on distances between data points, features with larger scales can dominate the distance calculation. Common methods include Min-Max scaling or Standardization (z-score normalization)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "93a95a6f",
   "metadata": {},
   "source": [
    "In addition to that : \n",
    "    Feature scaling plays a crucial role in KNN due to its reliance on distance metrics. Here's why feature scaling is important:\n",
    "\n",
    "Equal Contribution: Features with larger scales can disproportionately influence the distance calculation. By scaling the features, each feature contributes more evenly to the distance metric, preventing bias towards features with larger scales.\n",
    "\n",
    "Distance Consistency: Feature scaling ensures that the distances between data points are consistent across all features. Without scaling, features with larger scales may have a more significant impact on the overall distance, potentially leading to inaccurate neighbor selection.\n",
    "\n",
    "Improved Convergence: In algorithms that involve optimization or convergence, feature scaling can help achieve faster convergence. This is particularly important in cases where distance metrics are used to guide the learning process.\n",
    "\n",
    "Avoidance of Numerical Instabilities: Feature scaling can help avoid numerical instability issues that may arise when working with algorithms that involve computations sensitive to the scale of the input features.\n",
    "\n",
    "Common methods for feature scaling in KNN include:\n",
    "\n",
    "Min-Max Scaling: Scales the features to a specific range, often between 0 and 1.\n",
    "Standardization (Z-score Normalization): Transforms the features to have a mean of 0 and a standard deviation of 1.\n",
    "In summary, feature scaling ensures that the KNN algorithm performs consistently across all features, leading to more reliable and accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f4d0e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
